{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"../data/fake reviews dataset.csv\")\n",
    "\n",
    "# Step 1: Feature Engineering\n",
    "df['text_length'] = df['text_'].apply(len)\n",
    "df['sentiment_score'] = df['text_'].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "df['unique_word_ratio'] = df['text_'].apply(lambda x: len(set(x.split())) / len(x.split()))\n",
    "\n",
    "# TF-IDF Vectorization for text features\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "tfidf_features = vectorizer.fit_transform(df['text_']).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_features, columns=[f'tfidf_{i}' for i in range(tfidf_features.shape[1])])\n",
    "df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "# Select features for clustering\n",
    "features = df[['text_length', 'rating', 'sentiment_score', 'unique_word_ratio'] + list(tfidf_df.columns)]\n",
    "\n",
    "# Step 2: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Optional Step: Dimensionality Reduction with PCA\n",
    "pca = PCA(n_components=5)\n",
    "reduced_features = pca.fit_transform(scaled_features)\n",
    "print('fit')\n",
    "# Step 3: Agglomerative Clustering\n",
    "# Configure the model to find 2 clusters (CG and OR)\n",
    "agglomerative = AgglomerativeClustering(n_clusters=2)\n",
    "clusters = agglomerative.fit_predict(reduced_features)\n",
    "\n",
    "# Step 4: Map clusters to predominant labels\n",
    "df['cluster'] = clusters\n",
    "cluster_mapping = {}\n",
    "for i in set(df['cluster']):\n",
    "    label_counts = df[df['cluster'] == i]['label'].value_counts()\n",
    "    predominant_label = label_counts.idxmax()\n",
    "    cluster_mapping[i] = predominant_label\n",
    "\n",
    "df['predicted_label'] = df['cluster'].map(cluster_mapping)\n",
    "\n",
    "# Step 5: Confusion Matrix\n",
    "conf_matrix = confusion_matrix(df['label'], df['predicted_label'], labels=['CG', 'OR'])\n",
    "\n",
    "# Display the Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 4903 15313]\n",
      " [ 3507 16709]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"../data/fake reviews dataset.csv\")\n",
    "\n",
    "# Step 1: Feature Engineering\n",
    "df['text_length'] = df['text_'].apply(len)\n",
    "df['sentiment_score'] = df['text_'].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "df['unique_word_ratio'] = df['text_'].apply(lambda x: len(set(x.split())) / len(x.split()))\n",
    "\n",
    "# TF-IDF Vectorization for text features\n",
    "vectorizer = TfidfVectorizer(max_features=50)  # Reduced max features for efficiency\n",
    "tfidf_features = vectorizer.fit_transform(df['text_']).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_features, columns=[f'tfidf_{i}' for i in range(tfidf_features.shape[1])])\n",
    "df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "# Select features for clustering\n",
    "features = df[['text_length', 'rating', 'sentiment_score', 'unique_word_ratio'] + list(tfidf_df.columns)]\n",
    "\n",
    "# Step 2: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Optional Step: Dimensionality Reduction with PCA\n",
    "pca = PCA(n_components=10)  # Increased components to capture more variance\n",
    "reduced_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Step 3: MiniBatchKMeans Clustering\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters=3, batch_size=1000, random_state=42)\n",
    "clusters = minibatch_kmeans.fit_predict(reduced_features)\n",
    "\n",
    "# Step 4: Map clusters to predominant labels\n",
    "df['cluster'] = clusters\n",
    "cluster_mapping = {}\n",
    "for i in set(df['cluster']):\n",
    "    label_counts = df[df['cluster'] == i]['label'].value_counts()\n",
    "    predominant_label = label_counts.idxmax()\n",
    "    cluster_mapping[i] = predominant_label\n",
    "\n",
    "df['predicted_label'] = df['cluster'].map(cluster_mapping)\n",
    "\n",
    "# Step 5: Confusion Matrix\n",
    "conf_matrix = confusion_matrix(df['label'], df['predicted_label'], labels=['CG', 'OR'])\n",
    "\n",
    "# Display the Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/malfridurannaeiriksdottir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 6863 13353]\n",
      " [ 4417 15799]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"../data/fake reviews dataset.csv\")\n",
    "\n",
    "# Step 1: Feature Engineering\n",
    "df['text_length'] = df['text_'].apply(len)\n",
    "df['sentiment_score'] = df['text_'].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "df['unique_word_ratio'] = df['text_'].apply(lambda x: len(set(x.split())) / len(x.split()))\n",
    "\n",
    "# Additional Features\n",
    "df['average_word_length'] = df['text_'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()))\n",
    "df['punctuation_density'] = df['text_'].apply(lambda x: sum(1 for char in x if char in string.punctuation) / len(x))\n",
    "df['stopword_ratio'] = df['text_'].apply(lambda x: sum(1 for word in x.split() if word.lower() in stop_words) / len(x.split()))\n",
    "df['uppercase_word_ratio'] = df['text_'].apply(lambda x: sum(1 for word in x.split() if word.isupper()) / len(x.split()))\n",
    "df['sentence_count'] = df['text_'].apply(lambda x: len(x.split('.')))\n",
    "df['average_sentence_length'] = df['text_'].apply(lambda x: len(x.split()) / (len(x.split('.')) + 1))  # +1 to avoid division by zero\n",
    "\n",
    "# TF-IDF Vectorization for text features\n",
    "vectorizer = TfidfVectorizer(max_features=50)  # Reduced max features for efficiency\n",
    "tfidf_features = vectorizer.fit_transform(df['text_']).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_features, columns=[f'tfidf_{i}' for i in range(tfidf_features.shape[1])])\n",
    "df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "# Select features for clustering\n",
    "features = df[['text_length', 'rating', 'sentiment_score', 'unique_word_ratio', 'average_word_length',\n",
    "               'punctuation_density', 'stopword_ratio', 'uppercase_word_ratio', 'sentence_count',\n",
    "               'average_sentence_length'] + list(tfidf_df.columns)]\n",
    "\n",
    "# Step 2: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Optional Step: Dimensionality Reduction with PCA\n",
    "pca = PCA(n_components=10)  # Increased components to capture more variance\n",
    "reduced_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Step 3: MiniBatchKMeans Clustering\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters=3, batch_size=10000, random_state=42)\n",
    "clusters = minibatch_kmeans.fit_predict(reduced_features)\n",
    "\n",
    "# Step 4: Map clusters to predominant labels\n",
    "df['cluster'] = clusters\n",
    "cluster_mapping = {}\n",
    "for i in set(df['cluster']):\n",
    "    label_counts = df[df['cluster'] == i]['label'].value_counts()\n",
    "    predominant_label = label_counts.idxmax()\n",
    "    cluster_mapping[i] = predominant_label\n",
    "\n",
    "df['predicted_label'] = df['cluster'].map(cluster_mapping)\n",
    "\n",
    "# Step 5: Confusion Matrix\n",
    "conf_matrix = confusion_matrix(df['label'], df['predicted_label'], labels=['CG', 'OR'])\n",
    "\n",
    "# Display the Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/malfridurannaeiriksdottir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Percentages):\n",
      "           Predicted CG  Predicted OR\n",
      "Actual CG     59.621092     40.378908\n",
      "Actual OR     49.767511     50.232489\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Assuming `df` is your DataFrame after clustering with the predicted labels\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"../data/fake reviews dataset.csv\")\n",
    "\n",
    "# Step 1: Feature Engineering (add the previous features here)\n",
    "df['text_length'] = df['text_'].apply(len)\n",
    "df['sentiment_score'] = df['text_'].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "df['unique_word_ratio'] = df['text_'].apply(lambda x: len(set(x.split())) / len(x.split()))\n",
    "df['average_word_length'] = df['text_'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()))\n",
    "df['punctuation_density'] = df['text_'].apply(lambda x: sum(1 for char in x if char in string.punctuation) / len(x))\n",
    "df['stopword_ratio'] = df['text_'].apply(lambda x: sum(1 for word in x.split() if word.lower() in stop_words) / len(x.split()))\n",
    "df['uppercase_word_ratio'] = df['text_'].apply(lambda x: sum(1 for word in x.split() if word.isupper()) / len(x.split()))\n",
    "df['sentence_count'] = df['text_'].apply(lambda x: len(x.split('.')))\n",
    "df['average_sentence_length'] = df['text_'].apply(lambda x: len(x.split()) / (len(x.split('.')) + 1))  # +1 to avoid division by zero\n",
    "\n",
    "# TF-IDF Vectorization for text features\n",
    "vectorizer = TfidfVectorizer(max_features=50)\n",
    "tfidf_features = vectorizer.fit_transform(df['text_']).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_features, columns=[f'tfidf_{i}' for i in range(tfidf_features.shape[1])])\n",
    "df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "# Select features for clustering\n",
    "features = df[['text_length', 'rating', 'sentiment_score', 'unique_word_ratio', 'average_word_length',\n",
    "               'punctuation_density', 'stopword_ratio', 'uppercase_word_ratio', 'sentence_count',\n",
    "               'average_sentence_length'] + list(tfidf_df.columns)]\n",
    "\n",
    "# Step 2: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Optional Step: Dimensionality Reduction with PCA\n",
    "pca = PCA(n_components=10)\n",
    "reduced_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Step 3: MiniBatchKMeans Clustering\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters=3, batch_size=1000, random_state=42)\n",
    "clusters = minibatch_kmeans.fit_predict(reduced_features)\n",
    "\n",
    "# Map clusters to predominant labels\n",
    "df['cluster'] = clusters\n",
    "cluster_mapping = {}\n",
    "for i in set(df['cluster']):\n",
    "    label_counts = df[df['cluster'] == i]['label'].value_counts()\n",
    "    predominant_label = label_counts.idxmax()\n",
    "    cluster_mapping[i] = predominant_label\n",
    "\n",
    "df['predicted_label'] = df['cluster'].map(cluster_mapping)\n",
    "\n",
    "# Step 5: Confusion Matrix with Percentages\n",
    "conf_matrix = confusion_matrix(df['label'], df['predicted_label'], labels=['CG', 'OR'])\n",
    "conf_matrix_percent = (conf_matrix / conf_matrix.sum(axis=1, keepdims=True)) * 100\n",
    "\n",
    "# Display the Confusion Matrix with Percentages\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix_percent, index=['Actual CG', 'Actual OR'], columns=['Predicted CG', 'Predicted OR'])\n",
    "print(\"Confusion Matrix (Percentages):\")\n",
    "print(conf_matrix_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/malfridurannaeiriksdottir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Percentages):\n",
      "           Predicted CG  Predicted OR\n",
      "Actual CG     83.325089     16.674911\n",
      "Actual OR     79.387614     20.612386\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"../data/fake reviews dataset.csv\")\n",
    "\n",
    "# Step 1: Feature Engineering (repeating previous features)\n",
    "df['text_length'] = df['text_'].apply(len)\n",
    "df['sentiment_score'] = df['text_'].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "df['unique_word_ratio'] = df['text_'].apply(lambda x: len(set(x.split())) / len(x.split()))\n",
    "df['average_word_length'] = df['text_'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()))\n",
    "df['punctuation_density'] = df['text_'].apply(lambda x: sum(1 for char in x if char in string.punctuation) / len(x))\n",
    "df['stopword_ratio'] = df['text_'].apply(lambda x: sum(1 for word in x.split() if word.lower() in stop_words) / len(x.split()))\n",
    "df['uppercase_word_ratio'] = df['text_'].apply(lambda x: sum(1 for word in x.split() if word.isupper()) / len(x.split()))\n",
    "df['sentence_count'] = df['text_'].apply(lambda x: len(x.split('.')))\n",
    "df['average_sentence_length'] = df['text_'].apply(lambda x: len(x.split()) / (len(x.split('.')) + 1))  # +1 to avoid division by zero\n",
    "\n",
    "# TF-IDF Vectorization for text features\n",
    "vectorizer = TfidfVectorizer(max_features=50)\n",
    "tfidf_features = vectorizer.fit_transform(df['text_']).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_features, columns=[f'tfidf_{i}' for i in range(tfidf_features.shape[1])])\n",
    "df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "# Select features for clustering\n",
    "features = df[['text_length', 'rating', 'sentiment_score', 'unique_word_ratio', 'average_word_length',\n",
    "               'punctuation_density', 'stopword_ratio', 'uppercase_word_ratio', 'sentence_count',\n",
    "               'average_sentence_length'] + list(tfidf_df.columns)]\n",
    "\n",
    "# Step 2: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Optional Step: Dimensionality Reduction with PCA\n",
    "pca = PCA(n_components=10)\n",
    "reduced_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Step 3: Gaussian Mixture Model Clustering\n",
    "gmm = GaussianMixture(n_components=2, covariance_type='tied', random_state=42)\n",
    "clusters = gmm.fit_predict(reduced_features)\n",
    "\n",
    "# Step 4: Map clusters to predominant labels\n",
    "df['cluster'] = clusters\n",
    "cluster_mapping = {}\n",
    "for i in set(df['cluster']):\n",
    "    label_counts = df[df['cluster'] == i]['label'].value_counts()\n",
    "    predominant_label = label_counts.idxmax()\n",
    "    cluster_mapping[i] = predominant_label\n",
    "\n",
    "df['predicted_label'] = df['cluster'].map(cluster_mapping)\n",
    "\n",
    "# Step 5: Confusion Matrix and Percentages\n",
    "conf_matrix = confusion_matrix(df['label'], df['predicted_label'], labels=['CG', 'OR'])\n",
    "conf_matrix_percent = (conf_matrix / conf_matrix.sum(axis=1, keepdims=True)) * 100\n",
    "\n",
    "# Display the Confusion Matrix with Percentages\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix_percent, index=['Actual CG', 'Actual OR'], columns=['Predicted CG', 'Predicted OR'])\n",
    "print(\"Confusion Matrix (Percentages):\")\n",
    "print(conf_matrix_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/malfridurannaeiriksdottir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "(ProtocolError('Connection aborted.', TimeoutError(60, 'Operation timed out')), '(Request ID: fa1f8ca0-8047-4660-aff8-71cef28df962)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/http/client.py:1349\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1349\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/http/client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/http/client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 277\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 60] Operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/urllib3/connectionpool.py:847\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    845\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 847\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/urllib3/util/retry.py:470\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/urllib3/util/util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/http/client.py:1349\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1349\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/http/client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/http/client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 277\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Optional Step: Dimensionality Reduction with PCA (optional based on feature count)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdistilbert-base-nli-mean-tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m bert_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     67\u001b[0m reduced_features \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_transform(bert_embeddings)\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:87\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     83\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cache_folder, model_name_or_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodules.json\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;66;03m# Download from hub with caching\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m         \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence-transformers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mignore_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflax_model.msgpack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrust_model.ot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtf_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodules.json\u001b[39m\u001b[38;5;124m'\u001b[39m)):    \u001b[38;5;66;03m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(model_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/sentence_transformers/util.py:442\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, revision, cache_dir, library_name, library_version, user_agent, ignore_files, use_auth_token)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m use_auth_token:\n\u001b[1;32m    440\u001b[0m     token \u001b[38;5;241m=\u001b[39m HfFolder\u001b[38;5;241m.\u001b[39mget_token()\n\u001b[0;32m--> 442\u001b[0m model_info \u001b[38;5;241m=\u001b[39m \u001b[43m_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m storage_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    445\u001b[0m     cache_dir, repo_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    446\u001b[0m )\n\u001b[1;32m    448\u001b[0m all_files \u001b[38;5;241m=\u001b[39m model_info\u001b[38;5;241m.\u001b[39msiblings\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/huggingface_hub/hf_api.py:2466\u001b[0m, in \u001b[0;36mHfApi.model_info\u001b[0;34m(self, repo_id, revision, timeout, securityStatus, files_metadata, expand, token)\u001b[0m\n\u001b[1;32m   2464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expand:\n\u001b[1;32m   2465\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m expand\n\u001b[0;32m-> 2466\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2467\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   2468\u001b[0m data \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/huggingface_hub/utils/_http.py:93\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     95\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/anaconda3/envs/myMLvenv/lib/python3.9/site-packages/requests/adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    686\u001b[0m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[0;31mConnectionError\u001b[0m: (ProtocolError('Connection aborted.', TimeoutError(60, 'Operation timed out')), '(Request ID: fa1f8ca0-8047-4660-aff8-71cef28df962)')"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"../data/fake reviews dataset.csv\")\n",
    "\n",
    "# Step 1: Feature Engineering (same as before)\n",
    "df['text_length'] = df['text_'].apply(len)\n",
    "df['sentiment_score'] = df['text_'].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "df['unique_word_ratio'] = df['text_'].apply(lambda x: len(set(x.split())) / len(x.split()))\n",
    "df['average_word_length'] = df['text_'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()))\n",
    "df['punctuation_density'] = df['text_'].apply(lambda x: sum(1 for char in x if char in string.punctuation) / len(x))\n",
    "df['stopword_ratio'] = df['text_'].apply(lambda x: sum(1 for word in x.split() if word.lower() in stop_words) / len(x.split()))\n",
    "df['uppercase_word_ratio'] = df['text_'].apply(lambda x: sum(1 for word in x.split() if word.isupper()) / len(x.split()))\n",
    "df['sentence_count'] = df['text_'].apply(lambda x: len(x.split('.')))\n",
    "df['average_sentence_length'] = df['text_'].apply(lambda x: len(x.split()) / (len(x.split('.')) + 1))\n",
    "\n",
    "# TF-IDF Vectorization for text features\n",
    "vectorizer = TfidfVectorizer(max_features=50)\n",
    "tfidf_features = vectorizer.fit_transform(df['text_']).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_features, columns=[f'tfidf_{i}' for i in range(tfidf_features.shape[1])])\n",
    "df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "# Select features for clustering\n",
    "features = df[['text_length', 'rating', 'sentiment_score', 'unique_word_ratio', 'average_word_length',\n",
    "               'punctuation_density', 'stopword_ratio', 'uppercase_word_ratio', 'sentence_count',\n",
    "               'average_sentence_length'] + list(tfidf_df.columns)]\n",
    "\n",
    "# Step 2: Feature Selection with Random Forest\n",
    "# We'll use a labeled subset for feature selection\n",
    "subset_df = df[['label'] + features.columns.tolist()].dropna()\n",
    "X_subset = subset_df[features.columns]\n",
    "y_subset = subset_df['label']\n",
    "\n",
    "# Train Random Forest on the subset\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_subset, y_subset)\n",
    "\n",
    "# Select top features based on importance\n",
    "feature_importances = pd.Series(rf.feature_importances_, index=features.columns)\n",
    "top_features = feature_importances.nlargest(10).index.tolist()  # Select top 10 features\n",
    "\n",
    "# Filter features for clustering\n",
    "selected_features = df[top_features]\n",
    "\n",
    "# Step 3: Standardize the selected features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(selected_features)\n",
    "\n",
    "# Optional Step: Dimensionality Reduction with PCA (optional based on feature count)\n",
    "pca = PCA(n_components=5)\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "bert_embeddings = model.encode(df['text_'].tolist())\n",
    "reduced_features = PCA(n_components=10).fit_transform(bert_embeddings)\n",
    "# Step 4: Gaussian Mixture Model Clustering on Selected Features\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "clusters = gmm.fit_predict(reduced_features)\n",
    "\n",
    "# Step 5: Map clusters to predominant labels\n",
    "df['cluster'] = clusters\n",
    "cluster_mapping = {}\n",
    "for i in set(df['cluster']):\n",
    "    label_counts = df[df['cluster'] == i]['label'].value_counts()\n",
    "    predominant_label = label_counts.idxmax()\n",
    "    cluster_mapping[i] = predominant_label\n",
    "\n",
    "df['predicted_label'] = df['cluster'].map(cluster_mapping)\n",
    "\n",
    "# Step 6: Confusion Matrix with Percentages\n",
    "conf_matrix = confusion_matrix(df['label'], df['predicted_label'], labels=['CG', 'OR'])\n",
    "conf_matrix_percent = (conf_matrix / conf_matrix.sum(axis=1, keepdims=True)) * 100\n",
    "\n",
    "# Display the Confusion Matrix with Percentages\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix_percent, index=['Actual CG', 'Actual OR'], columns=['Predicted CG', 'Predicted OR'])\n",
    "print(\"Confusion Matrix (Percentages):\")\n",
    "print(conf_matrix_df)\n",
    "\n",
    "# Display Selected Top Features\n",
    "print(\"Top Features Selected for Clustering:\")\n",
    "print(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.25.1\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl.metadata (93 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m384.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentence-transformers==2.2.2\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.25.1) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.25.1) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.25.1) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.25.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.25.1) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.25.1) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.25.1) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.25.1)\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-macosx_12_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.25.1) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (2.3.1)\n",
      "Requirement already satisfied: torchvision in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (0.18.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (1.4.0)\n",
      "Requirement already satisfied: scipy in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (1.12.0)\n",
      "Requirement already satisfied: nltk in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (0.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.3)\n",
      "Requirement already satisfied: click in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from nltk->sentence-transformers==2.2.2) (1.3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from requests->transformers==4.25.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from requests->transformers==4.25.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from requests->transformers==4.25.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from requests->transformers==4.25.1) (2024.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from torchvision->sentence-transformers==2.2.2) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
      "Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m733.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.13.3-cp39-cp39-macosx_12_0_arm64.whl (3.9 MB)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=3c5332afd0732c83a55697aca3ad50bb90691d8d0d0d9fa179edd4995dfe49e0\n",
      "  Stored in directory: /Users/malfridurannaeiriksdottir/Library/Caches/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.46.2\n",
      "    Uninstalling transformers-4.46.2:\n",
      "      Successfully uninstalled transformers-4.46.2\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 3.3.0\n",
      "    Uninstalling sentence-transformers-3.3.0:\n",
      "      Successfully uninstalled sentence-transformers-3.3.0\n",
      "Successfully installed sentence-transformers-2.2.2 tokenizers-0.13.3 transformers-4.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.25.1 sentence-transformers==2.2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (4.33.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: sentence-transformers in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (3.3.0)\n",
      "Requirement already satisfied: filelock in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied: scipy in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: Pillow in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Using cached transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "Downloading tokenizers-0.20.3-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m438.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.33.1\n",
      "    Uninstalling transformers-4.33.1:\n",
      "      Successfully uninstalled transformers-4.33.1\n",
      "Successfully installed tokenizers-0.20.3 transformers-4.46.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U transformers sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.14.1\n",
      "  Downloading tensorflow-2.14.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting transformers==4.33.1\n",
      "  Downloading transformers-4.33.1-py3-none-any.whl.metadata (119 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-macos==2.14.1 (from tensorflow==2.14.1)\n",
      "  Downloading tensorflow_macos-2.14.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: filelock in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.33.1) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.33.1) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.33.1) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.33.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.33.1) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.33.1) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.33.1) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.1)\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-macosx_12_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.33.1) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from transformers==4.33.1) (4.66.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorflow-macos==2.14.1->tensorflow==2.14.1) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorflow-macos==2.14.1->tensorflow==2.14.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorflow-macos==2.14.1->tensorflow==2.14.1) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorflow-macos==2.14.1->tensorflow==2.14.1) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorflow-macos==2.14.1->tensorflow==2.14.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorflow-macos==2.14.1->tensorflow==2.14.1) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorflow-macos==2.14.1->tensorflow==2.14.1) (18.1.1)\n",
      "Collecting ml-dtypes==0.2.0 (from tensorflow-macos==2.14.1->tensorflow==2.14.1)\n",
      "  Downloading ml_dtypes-0.2.0-cp39-cp39-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorflow-macos==2.14.1->tensorflow==2.14.1) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorflow-macos==2.14.1->tensorflow==2.14.1) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorflow-macos==2.14.1->tensorflow==2.14.1) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorflow-macos==2.14.1->tensorflow==2.14.1) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorflow-macos==2.14.1->tensorflow==2.14.1) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorflow-macos==2.14.1->tensorflow==2.14.1) (4.9.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-macos==2.14.1->tensorflow==2.14.1)\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorflow-macos==2.14.1->tensorflow==2.14.1) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorflow-macos==2.14.1->tensorflow==2.14.1) (1.62.1)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow-macos==2.14.1->tensorflow==2.14.1)\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow-macos==2.14.1->tensorflow==2.14.1)\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow-macos==2.14.1->tensorflow==2.14.1)\n",
      "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.1) (2023.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from requests->transformers==4.33.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from requests->transformers==4.33.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from requests->transformers==4.33.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from requests->transformers==4.33.1) (2024.2.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.14.1->tensorflow==2.14.1) (0.41.2)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.1->tensorflow==2.14.1)\n",
      "  Downloading google_auth-2.36.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.1->tensorflow==2.14.1)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.1->tensorflow==2.14.1) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.1->tensorflow==2.14.1) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.1->tensorflow==2.14.1) (3.0.1)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.1->tensorflow==2.14.1)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.1->tensorflow==2.14.1)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.1->tensorflow==2.14.1)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.1->tensorflow==2.14.1)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.1->tensorflow==2.14.1) (4.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.1->tensorflow==2.14.1) (2.1.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/malfridurannaeiriksdottir/anaconda3/envs/myMLvenv/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.1->tensorflow==2.14.1) (3.17.0)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.1->tensorflow==2.14.1)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.1->tensorflow==2.14.1)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading tensorflow-2.14.1-cp39-cp39-macosx_12_0_arm64.whl (2.1 kB)\n",
      "Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_macos-2.14.1-cp39-cp39-macosx_12_0_arm64.whl (199.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m199.7/199.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0mm\n",
      "\u001b[?25hDownloading ml_dtypes-0.2.0-cp39-cp39-macosx_10_9_universal2.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading tokenizers-0.13.3-cp39-cp39-macosx_12_0_arm64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.14.1-cp39-cp39-macosx_11_0_arm64.whl (35 kB)\n",
      "Downloading google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m209.5/209.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, wrapt, tensorflow-estimator, pyasn1, oauthlib, ml-dtypes, keras, cachetools, rsa, requests-oauthlib, pyasn1-modules, transformers, google-auth, google-auth-oauthlib, tensorboard, tensorflow-macos, tensorflow\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.0\n",
      "    Uninstalling tokenizers-0.20.0:\n",
      "      Successfully uninstalled tokenizers-0.20.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.3.2\n",
      "    Uninstalling ml-dtypes-0.3.2:\n",
      "      Successfully uninstalled ml-dtypes-0.3.2\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.1.1\n",
      "    Uninstalling keras-3.1.1:\n",
      "      Successfully uninstalled keras-3.1.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.1\n",
      "    Uninstalling transformers-4.45.1:\n",
      "      Successfully uninstalled transformers-4.45.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.16.2\n",
      "    Uninstalling tensorboard-2.16.2:\n",
      "      Successfully uninstalled tensorboard-2.16.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.16.1\n",
      "    Uninstalling tensorflow-2.16.1:\n",
      "      Successfully uninstalled tensorflow-2.16.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 3.3.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.33.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cachetools-5.5.0 google-auth-2.36.0 google-auth-oauthlib-1.0.0 keras-2.14.0 ml-dtypes-0.2.0 oauthlib-3.2.2 pyasn1-0.6.1 pyasn1-modules-0.4.1 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.14.1 tensorflow-2.14.1 tensorflow-estimator-2.14.0 tensorflow-macos-2.14.1 tokenizers-0.13.3 transformers-4.33.1 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.14.1 transformers==4.33.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/fake reviews dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  rating label  \\\n",
       "0  Home_and_Kitchen_5       5    CG   \n",
       "1  Home_and_Kitchen_5       5    CG   \n",
       "2  Home_and_Kitchen_5       5    CG   \n",
       "3  Home_and_Kitchen_5       1    CG   \n",
       "4  Home_and_Kitchen_5       5    CG   \n",
       "\n",
       "                                               text_  \n",
       "0  Love this!  Well made, sturdy, and very comfor...  \n",
       "1  love it, a great upgrade from the original.  I...  \n",
       "2  This pillow saved my back. I love the look and...  \n",
       "3  Missing information on how to use it, but it i...  \n",
       "4  Very nice set. Good quality. We have had the s...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myMLvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
